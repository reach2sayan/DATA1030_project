{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Basic Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/data_pp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asses the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['billboard'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping ID, AlbumId, date_month and date_day\n",
    "\n",
    "df_fit = df.drop(['date_day','id','album_id','date_month','decade'],axis=1) # as mentioned in the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Feature and Target Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing data\n",
    "\n",
    "X = df_fit.drop(['billboard'],axis=1)\n",
    "y = df_fit['billboard']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the Cross-Validation and Train Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=plt.cm.coolwarm,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "               c=y, marker='_', lw=lw, cmap=plt.cm.Paired)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + ['class']\n",
    "    ax.set(yticks=np.arange(n_splits+1) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits+1.1, -.1], xlim=[0, 100])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax\n",
    "\n",
    "groups = [1]*len(X)\n",
    "fig, ax = plt.subplots()\n",
    "cv = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "ax = plot_cv_indices(cv, X, y, groups, ax, n_splits=5)\n",
    "plt.savefig('../figures/CV_split.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary Pipeline, the successive methods were just commented/uncommented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def ML_pipeline_GridSearchCV(X,y,random_state,n_folds):\n",
    "    \n",
    "    # create a test set based on groups\n",
    "\n",
    "    X_other, X_test, y_other, y_test = train_test_split(X, y, test_size=0.2, random_state = random_state, stratify=y)\n",
    "    \n",
    "    # create the cross validator\n",
    "    kf = StratifiedKFold(n_splits=n_folds,shuffle=True,random_state=random_state)\n",
    "    \n",
    "    # create the pipeline: preprocessor + supervised ML method\n",
    "    \n",
    "    #pipe_SVC = make_pipeline(SVC())\n",
    "    #pipe_RandomForest = make_pipeline(RandomForestClassifier(n_estimators=100,random_state=random_state))\n",
    "    #pipe_LogisticRegression = make_pipeline(LogisticRegression(solver='saga',max_iter=10000,random_state=random_state))\n",
    "    #pipe_KNN = make_pipeline(KNeighborsClassifier(n_jobs=-1))\n",
    "    #pipe_LDA = make_pipeline(LDA())\n",
    "    #pipe_adaboost = make_pipeline(AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2),n_estimators=100,random_state=random_state))\n",
    "    #pipe_xgboost = make_pipeline(XGBClassifier(n_estimators=100, objective='binary:logistic', learning_rate=0.3, colsample_bytree=0.9, subsample=0.66,\n",
    "    #               silent=False, nthread=1))\n",
    "    \n",
    "    # the parameter(s) we want to tune\n",
    "    \n",
    "    #param_grid_SVC = {'svc__C': np.logspace(-3,4,num=8),'svc__gamma': np.logspace(-3,4,num=8)}\n",
    "    #param_grid_randomForest = {'randomforestclassifier__max_depth': np.linspace(2,10,num=9,dtype=int),'randomforestclassifier__min_samples_split': np.linspace(2,10,num=9,dtype=int)}\n",
    "    #param_grid_logisticregression = {'logisticregression__penalty': ['l1','l2'],'logisticregression__C': np.logspace(-4, 4, 20)}\n",
    "    #param_grid_knn = {'kneighborsclassifier__n_neighbors': np.linspace(1,100,20,dtype=int),'kneighborsclassifier__weights': ['uniform','distance'], 'kneighborsclassifier__metric': ['euclidean','manhattan']}\n",
    "    #param_grid_LDA = {'lineardiscriminantanalysis__solver': ['eigen','lsqr','svd']}\n",
    "    #param_grid_adaboost = {'adaboostclassifier__learning_rate': np.logspace(-3,4,num=8), 'adaboostclassifier__algorithm': ['SAMME', 'SAMME.R']}\n",
    "    #param_grid_xgboost = {\"xgbclassifier__max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    "    #                      \"xgbclassifier__min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "    #                      \"xgbclassifier__gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ]}\n",
    "    \n",
    "    # prepare gridsearch\n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid,scoring = make_scorer(accuracy_score),\n",
    "                        cv=kf, return_train_score = True,iid=False, verbose=3, n_jobs=-1)\n",
    "    # do kfold CV on _other\n",
    "    \n",
    "    grid.fit(X_other, y_other)\n",
    "    \n",
    "    return grid, grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now call each method and save them for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SVC-rbf didn't converge for 5 hours\n",
    "## KNN didn't converge after 12 hours\n",
    "## Random Forest saved\n",
    "## logistic regression saved\n",
    "## LDA saved\n",
    "## QDA saved\n",
    "## AdaBoost saved\n",
    "import pickle\n",
    "    \n",
    "test_scores = []\n",
    "for i in range(5):\n",
    "    grid, test_score = ML_pipeline_GridSearchCV(X,y,i*42,5)\n",
    "    print(grid.best_params_)\n",
    "    print('best CV score:',grid.best_score_)\n",
    "    print('test score:',test_score)\n",
    "    \n",
    "    fname = '../results/grid'+str(i)+'.lasso'\n",
    "    file = open(fname, 'wb')\n",
    "    pickle.dump(grid,file)\n",
    "    file.close()\n",
    "    test_scores.append(test_score)\n",
    "    \n",
    "print('test accuracy:',np.around(np.mean(test_scores),2),'+/-',np.around(np.std(test_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some LDA and QDA just for completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Non Parametric Classification\n",
    "\n",
    "def ML_pipeline_nonParametric(X,y,random_state):\n",
    "    \n",
    "    X_other, X_test, y_other, y_test = train_test_split(X, y, test_size=0.2, random_state = random_state, stratify=y)\n",
    "    clf = QDA() # Replace with LDA\n",
    "    clf.fit(X_other,y_other)\n",
    "    return clf, clf.score(X_test,y_test)\n",
    "    # splitter for _other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now run the algorithms from this cell and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = []\n",
    "for i in range(5):\n",
    "    clf, test_score = ML_pipeline_nonParametric(X,y,i*42)\n",
    "    fname = '../results/grid'+str(i)+'.qda'\n",
    "    file = open(fname, 'wb')\n",
    "    pickle.dump(grid,file)\n",
    "    file.close()\n",
    "    test_scores.append(test_score)\n",
    "    \n",
    "print('test accuracy:',np.around(np.mean(test_scores),2),'+/-',np.around(np.std(test_scores),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
